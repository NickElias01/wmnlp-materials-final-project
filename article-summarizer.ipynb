{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Nick Elias\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract html from coffee review article website\n",
    "\n",
    "import requests\n",
    "\n",
    "def save_article_html(url, filename):\n",
    "    # Send the HTTP request to the URL\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the HTML content of the page\n",
    "        html_content = response.text\n",
    "\n",
    "        # Save the HTML content to a file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"HTML content saved to {filename}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page.\")\n",
    "\n",
    "# Example: Save HTML content of a coffee maker review article\n",
    "url = 'https://www.nytimes.com/wirecutter/reviews/best-espresso-machine-grinder-and-accessories-for-beginners/'  # Replace with the actual review article URL\n",
    "save_article_html(url, 'coffee_maker_review.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Load the spaCy model and add SpacyTextBlob extension\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def extract_clean_text_and_analyze_sentiment(html_file):\n",
    "    # Read the HTML file\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove <script> and <style> elements\n",
    "    for script_or_style in soup(['script', 'style']):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    # Optionally remove nav and footer sections\n",
    "    for nav_or_footer in soup.find_all(['nav', 'footer']):\n",
    "        nav_or_footer.decompose()\n",
    "\n",
    "    # Extract main article content (if applicable)\n",
    "    main_content = soup.find('article') or soup.find('div', class_='main-content')\n",
    "    if main_content:\n",
    "        article_text = main_content.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Clean up whitespace and HTML entities\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)  # Normalize spaces\n",
    "    article_text = unescape(article_text)  # Decode HTML entities\n",
    "\n",
    "    # Remove specific unwanted phrases like \"Advertisement SKIP ADVERTISEMENT\"\n",
    "    article_text = re.sub(r'\\bAdvertisement\\s*SKIP\\s*ADVERTISEMENT\\b', '', article_text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Process the cleaned text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # Get the polarity score (sentiment analysis) from SpacyTextBlob\n",
    "    polarity = doc._.blob.polarity  # SpacyTextBlob stores polarity in the ._.polarity attribute\n",
    "\n",
    "    # Count the number of sentences in the article\n",
    "    num_sentences = len(list(doc.sents))\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Sentiment Polarity: {polarity:.2f} (Positive if >0, Negative if <0, Neutral if =0)\")\n",
    "    print(f\"Number of Sentences in the article: {num_sentences}\")\n",
    "\n",
    "    # Optionally, save the cleaned text to a .txt file\n",
    "    output_file = 'cleaned_coffee_maker_review.txt'  # Desired file name\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(f\"Cleaned text has been saved to {output_file}\")\n",
    "\n",
    "    # Save the cleaned HTML back to a new HTML file\n",
    "    cleaned_html_file = 'cleaned_coffee_maker_review.html'  # Desired HTML file name\n",
    "    with open(cleaned_html_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(soup))  # Write the cleaned soup (HTML) to file\n",
    "    print(f\"Cleaned HTML has been saved to {cleaned_html_file}\")\n",
    "\n",
    "# Example: Analyze sentiment and sentences for the saved article HTML\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "extract_clean_text_and_analyze_sentiment(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Frequent Tokens:\n",
      "espresso: 106\n",
      "machine: 94\n",
      "breville: 55\n",
      "shots: 54\n",
      "barista: 48\n",
      "\n",
      "Token Frequencies:\n",
      "4: 1\n",
      "best: 19\n",
      "home: 17\n",
      "espresso: 106\n",
      "machines: 43\n",
      "2024: 5\n",
      "|: 1\n",
      "reviews: 2\n",
      "wirecutter: 31\n",
      "advertisement: 18\n",
      "skip: 10\n",
      "content: 1\n",
      "help: 8\n",
      "decide: 1\n",
      "research: 2\n",
      "log: 1\n",
      "account: 1\n",
      "independently: 1\n",
      "review: 3\n",
      "recommend: 11\n",
      "buy: 9\n",
      "links: 1\n",
      "earn: 1\n",
      "commission: 1\n",
      "learn: 6\n",
      "deal: 7\n",
      "breville: 55\n",
      "barista: 48\n",
      "touch: 45\n",
      "machine: 94\n",
      "$: 40\n",
      "693: 5\n",
      "27: 2\n",
      "kitchen: 7\n",
      "coffee: 48\n",
      "updated: 3\n",
      "november: 2\n",
      "25: 1\n",
      "save: 5\n",
      "connie: 19\n",
      "park: 19\n",
      "nyt: 23\n",
      "justin: 3\n",
      "vassallo: 2\n",
      "ciara: 4\n",
      "murray: 4\n",
      "jordan: 4\n",
      "fyi: 1\n",
      "new: 8\n",
      "round: 3\n",
      "testing: 10\n",
      "profitec: 26\n",
      "pick: 26\n",
      "oracle: 29\n",
      "jet: 29\n",
      "upgrade: 9\n",
      "gaggia: 25\n",
      "classic: 21\n",
      "evo: 11\n",
      "pro: 21\n",
      "making: 21\n",
      "cafÃ©: 11\n",
      "quality: 13\n",
      "drinks: 14\n",
      "longer: 5\n",
      "reserved: 1\n",
      "hobbyists: 2\n",
      "baristas: 5\n",
      "easier: 2\n",
      "beginner: 3\n",
      "started: 1\n",
      "little: 14\n",
      "practice: 8\n",
      "lattes: 5\n",
      "cortados: 3\n",
      "rival: 1\n",
      "upscale: 1\n",
      "changing: 2\n",
      "pajamas: 1\n",
      "dozens: 3\n",
      "think: 10\n",
      "option: 6\n",
      "skilled: 3\n",
      "enthusiasts: 1\n",
      "alike: 2\n",
      "powerful: 14\n",
      "easy: 11\n",
      "use: 21\n",
      "capable: 6\n",
      "yielding: 1\n",
      "consistent: 19\n",
      "rich: 1\n",
      "shots: 54\n",
      "robust: 2\n",
      "flavor: 8\n",
      "profiles: 2\n",
      "pull: 18\n",
      "deep: 3\n",
      "flavorful: 5\n",
      "combining: 3\n",
      "capability: 3\n",
      "higher: 3\n",
      "priced: 3\n",
      "basic: 5\n",
      "interface: 3\n",
      "simple: 6\n",
      "beginners: 12\n",
      "build: 5\n",
      "power: 8\n",
      "appeal: 3\n",
      "pros: 3\n",
      "buying: 16\n",
      "options: 18\n",
      "1,059: 6\n",
      "latte: 6\n",
      "love: 6\n",
      "clive: 3\n",
      "runner: 6\n",
      "automated: 5\n",
      "sleek: 5\n",
      "great: 16\n",
      "programming: 4\n",
      "solid: 7\n",
      "built: 17\n",
      "grinder: 42\n",
      "lets: 7\n",
      "variety: 4\n",
      "learning: 6\n",
      "curve: 4\n",
      "950: 4\n",
      "amazon: 9\n",
      "stainless: 5\n",
      "steel: 5\n",
      "899: 4\n",
      "walmart: 4\n",
      "1,000: 4\n",
      "budget: 8\n",
      "nuanced: 7\n",
      "entry: 5\n",
      "level: 9\n",
      "price: 8\n",
      "affordable: 3\n",
      "yield: 5\n",
      "surprisingly: 4\n",
      "complex: 7\n",
      "need: 17\n",
      "separate: 3\n",
      "tamper: 22\n",
      "frothing: 14\n",
      "milk: 46\n",
      "bit: 10\n",
      "tricky: 8\n",
      "microfoam: 10\n",
      "manageable: 3\n",
      "457: 3\n",
      "549: 3\n",
      "sur: 5\n",
      "la: 5\n",
      "table: 5\n",
      "ultimate: 2\n",
      "semi: 5\n",
      "automatic: 20\n",
      "grinds: 4\n",
      "doses: 6\n",
      "tamps: 4\n",
      "froths: 2\n",
      "guides: 4\n",
      "entire: 5\n",
      "process: 10\n",
      "2,000: 6\n",
      "heavy: 3\n",
      "impressive: 4\n",
      "pulls: 5\n",
      "range: 6\n",
      "depth: 5\n",
      "dialed: 3\n",
      "highlight: 1\n",
      "particular: 2\n",
      "characteristics: 1\n",
      "beans: 11\n",
      "straightforward: 3\n",
      "attractive: 1\n",
      "design: 6\n",
      "comfortable: 6\n",
      "portafilter: 24\n",
      "locks: 1\n",
      "group: 4\n",
      "head: 5\n",
      "easily: 5\n",
      "steam: 38\n",
      "wand: 28\n",
      "angled: 1\n",
      "wide: 1\n",
      "heights: 1\n",
      "directions: 1\n",
      "especially: 5\n",
      "takes: 10\n",
      "beautiful: 3\n",
      "silky: 4\n",
      "foam: 3\n",
      "seriously: 1\n",
      "interested: 2\n",
      "serve: 2\n",
      "novice: 1\n",
      "offers: 3\n",
      "lots: 1\n",
      "guidance: 3\n",
      "form: 1\n",
      "touchscreen: 8\n",
      "control: 10\n",
      "center: 3\n",
      "step: 10\n",
      "tutorials: 3\n",
      "multiple: 3\n",
      "programs: 5\n",
      "includes: 4\n",
      "advanced: 2\n",
      "controls: 2\n",
      "allows: 14\n",
      "manual: 7\n",
      "operation: 1\n",
      "experienced: 2\n",
      "users: 2\n",
      "people: 4\n",
      "want: 17\n",
      "creative: 1\n",
      "good: 15\n",
      "adjustable: 1\n",
      "auto: 4\n",
      "froth: 13\n",
      "setting: 8\n",
      "created: 2\n",
      "choice: 1\n",
      "dive: 1\n",
      "right: 6\n",
      "start: 4\n",
      "decent: 6\n",
      "hands: 1\n",
      "experience: 5\n",
      "version: 1\n",
      "popular: 1\n",
      "decades: 1\n",
      "thanks: 1\n",
      "approachable: 1\n",
      "ability: 4\n",
      "produce: 6\n",
      "worthy: 1\n",
      "somewhat: 1\n",
      "improved: 1\n",
      "compared: 2\n",
      "weaker: 1\n",
      "creating: 1\n",
      "velvety: 2\n",
      "texture: 3\n",
      "time: 10\n",
      "master: 2\n",
      "picks: 15\n",
      "able: 6\n",
      "greater: 2\n",
      "nuance: 2\n",
      "taste: 1\n",
      "high: 5\n",
      "spare: 1\n",
      "strongly: 1\n",
      "replace: 2\n",
      "flimsy: 4\n",
      "plastic: 4\n",
      "comes: 11\n",
      "work: 2\n",
      "unlike: 7\n",
      "super: 4\n",
      "teaches: 1\n",
      "better: 12\n",
      "walking: 1\n",
      "steps: 3\n",
      "detecting: 1\n",
      "small: 12\n",
      "changes: 1\n",
      "required: 1\n",
      "dial: 9\n",
      "shot: 44\n",
      "like: 19\n",
      "drink: 9\n",
      "choose: 3\n",
      "non: 5\n",
      "dairy: 3\n",
      "houseguest: 1\n",
      "walk: 2\n",
      "end: 1\n",
      "pretty: 2\n",
      "trust: 2\n",
      "picked: 2\n",
      "tested: 12\n",
      "worth: 3\n",
      "considering: 3\n",
      "choosing: 2\n",
      "competition: 2\n",
      "sources: 2\n",
      "associate: 2\n",
      "staff: 2\n",
      "writer: 3\n",
      "covering: 1\n",
      "gear: 2\n",
      "nespresso: 1\n",
      "frothers: 1\n",
      "tasting: 4\n",
      "espressos: 1\n",
      "familiar: 2\n",
      "expect: 2\n",
      "preparation: 2\n",
      "guide: 5\n",
      "took: 7\n",
      "class: 1\n",
      "project: 8\n",
      "ny: 7\n",
      "sensory: 1\n",
      "evaluation: 1\n",
      "course: 4\n",
      "vasallo: 1\n",
      "worked: 2\n",
      "previous: 2\n",
      "versions: 1\n",
      "lead: 3\n",
      "10: 2\n",
      "years: 2\n",
      "volume: 3\n",
      "shops: 1\n",
      "york: 2\n",
      "city: 3\n",
      "boston: 1\n",
      "read: 2\n",
      "articles: 1\n",
      "blog: 1\n",
      "posts: 1\n",
      "experts: 2\n",
      "watched: 1\n",
      "product: 3\n",
      "demo: 1\n",
      "videos: 1\n",
      "sites: 1\n",
      "seattle: 1\n",
      "sell: 1\n",
      "equipment: 1\n",
      "2021: 2\n",
      "interviewed: 2\n",
      "chisum: 2\n",
      "ngai: 2\n",
      "kaleena: 6\n",
      "teoh: 6\n",
      "past: 1\n",
      "beverage: 3\n",
      "category: 3\n",
      "brought: 4\n",
      "lineup: 1\n",
      "founder: 2\n",
      "tried: 2\n",
      "provided: 2\n",
      "professional: 1\n",
      "opinion: 1\n",
      "journalists: 1\n",
      "test: 1\n",
      "products: 1\n",
      "complete: 1\n",
      "editorial: 3\n",
      "independence: 1\n",
      "aware: 1\n",
      "business: 2\n",
      "implications: 1\n",
      "recommendations: 3\n",
      "standards: 1\n",
      "commitment: 1\n",
      "obsessed: 1\n",
      "investment: 2\n",
      "quickly: 9\n",
      "proves: 1\n",
      "dedicated: 2\n",
      "drinker: 1\n",
      "real: 1\n",
      "bet: 1\n",
      "way: 3\n",
      "true: 2\n",
      "quickest: 1\n",
      "ways: 1\n",
      "cup: 4\n",
      "morning: 2\n",
      "heat: 8\n",
      "kettle: 1\n",
      "boil: 1\n",
      "dosing: 2\n",
      "tamping: 7\n",
      "require: 2\n",
      "modicum: 1\n",
      "effort: 2\n",
      "minute: 4\n",
      "grinding: 6\n",
      "sitting: 1\n",
      "hot: 7\n",
      "crema: 1\n",
      "topped: 1\n",
      "prioritized: 1\n",
      "deliver: 2\n",
      "delicious: 2\n",
      "tinkering: 1\n",
      "novices: 2\n",
      "find: 3\n",
      "drinkers: 2\n",
      "happy: 1\n",
      "style: 1\n",
      "cappuccinos: 2\n",
      "recreate: 1\n",
      "favorite: 2\n",
      "milky: 1\n",
      "beverages: 1\n",
      "comfort: 1\n",
      "tailor: 1\n",
      "liking: 1\n",
      "blend: 6\n",
      "adjusting: 3\n",
      "brewing: 8\n",
      "factors: 2\n",
      "bring: 3\n",
      "certain: 2\n",
      "flavors: 5\n",
      "steaming: 6\n",
      "exact: 2\n",
      "sounds: 1\n",
      "intimidating: 1\n",
      "designed: 4\n",
      "hold: 8\n",
      "hand: 5\n",
      "standout: 1\n",
      "push: 2\n",
      "button: 6\n",
      "pulling: 12\n",
      "sarah: 9\n",
      "kobos: 9\n",
      "cheap: 2\n",
      "toylike: 1\n",
      "hulking: 1\n",
      "costing: 2\n",
      "thousands: 1\n",
      "2016: 2\n",
      "models: 7\n",
      "300: 2\n",
      "trying: 2\n",
      "sweet: 2\n",
      "spot: 1\n",
      "obsessives: 1\n",
      "enjoy: 1\n",
      "offer: 5\n",
      "value: 3\n",
      "features: 8\n",
      "baseline: 1\n",
      "works: 2\n",
      "forcing: 1\n",
      "water: 21\n",
      "finely: 4\n",
      "ground: 5\n",
      "pressure: 10\n",
      "ones: 3\n",
      "temperature: 14\n",
      "195: 1\n",
      "205: 1\n",
      "degrees: 2\n",
      "fahrenheit: 3\n",
      "apply: 1\n",
      "flows: 1\n",
      "grounds: 9\n",
      "evenly: 4\n",
      "balanced: 10\n",
      "extraction: 11\n",
      "ease: 4\n",
      "transitions: 1\n",
      "luxe: 2\n",
      "parts: 1\n",
      "accessories: 4\n",
      "following: 2\n",
      "criteria: 1\n",
      "narrow: 1\n",
      "considered: 2\n",
      "pump: 4\n",
      "create: 2\n",
      "allow: 8\n",
      "adjust: 6\n",
      "grind: 30\n",
      "size: 12\n",
      "ratio: 1\n",
      "balance: 3\n",
      "prefer: 3\n",
      "lever: 5\n",
      "hard: 2\n",
      "leading: 1\n",
      "over-: 2\n",
      "extracted: 4\n",
      "jura: 1\n",
      "measure: 1\n",
      "room: 2\n",
      "experimentation: 3\n",
      "pricey: 2\n",
      "smallest: 1\n",
      "left: 2\n",
      "biggest: 1\n",
      "single: 18\n",
      "boiler: 13\n",
      "mean: 1\n",
      "waiting: 2\n",
      "minutes: 4\n",
      "technology: 3\n",
      "wait: 2\n",
      "dual: 5\n",
      "simultaneously: 1\n",
      "expensive: 4\n",
      "wo: 6\n",
      "entails: 1\n",
      "multitasking: 1\n",
      "usually: 1\n",
      "necessary: 2\n",
      "fast: 2\n",
      "heater: 2\n",
      "speedy: 1\n",
      "provides: 3\n",
      "fun: 1\n",
      "rhythm: 1\n",
      "promises: 1\n",
      "daily: 1\n",
      "ritual: 1\n",
      "pid: 7\n",
      "proportional: 1\n",
      "integral: 2\n",
      "derivative: 1\n",
      "controllers: 1\n",
      "regulate: 2\n",
      "allowing: 2\n",
      "thermojet: 2\n",
      "heaters: 1\n",
      "transition: 4\n",
      "preparations: 1\n",
      "barely: 3\n",
      "finish: 2\n",
      "strong: 4\n",
      "properly: 6\n",
      "extract: 3\n",
      "packed: 1\n",
      "dose: 6\n",
      "free: 1\n",
      "big: 2\n",
      "bubbles: 2\n",
      "key: 2\n",
      "component: 1\n",
      "needs: 2\n",
      "distributed: 1\n",
      "compacted: 1\n",
      "order: 2\n",
      "path: 1\n",
      "resistance: 1\n",
      "weak: 2\n",
      "point: 1\n",
      "puck: 7\n",
      "flow: 1\n",
      "flowing: 1\n",
      "called: 1\n",
      "channeling: 1\n",
      "causes: 2\n",
      "unbalanced: 2\n",
      "cover: 3\n",
      "width: 2\n",
      "filter: 12\n",
      "basket: 10\n",
      "handle: 2\n",
      "base: 3\n",
      "aids: 1\n",
      "exerting: 1\n",
      "preventing: 1\n",
      "points: 2\n",
      "friendly: 2\n",
      "effective: 3\n",
      "angle: 4\n",
      "pitcher: 12\n",
      "thing: 3\n",
      "nuances: 1\n",
      "distinguish: 1\n",
      "excellent: 2\n",
      "ultimately: 1\n",
      "achieved: 1\n",
      "manually: 6\n",
      "online: 1\n",
      "video: 2\n",
      "consider: 2\n",
      "flaw: 1\n",
      "programmable: 2\n",
      "come: 5\n",
      "programmed: 1\n",
      "settings: 16\n",
      "vary: 1\n",
      "found: 6\n",
      "starting: 3\n",
      "stopping: 2\n",
      "alongside: 1\n",
      "tweaking: 1\n",
      "nice: 3\n",
      "and/or: 1\n",
      "timing: 3\n",
      "streamline: 1\n",
      "routine: 1\n",
      "getting: 6\n",
      "latest: 1\n",
      "pulled: 9\n",
      "tasted: 2\n",
      "dialing: 5\n",
      "baratza: 13\n",
      "sette: 19\n",
      "270: 10\n",
      "ninth: 1\n",
      "street: 1\n",
      "alphabet: 1\n",
      "blue: 1\n",
      "bottle: 1\n",
      "hayes: 1\n",
      "valley: 1\n",
      "grumpy: 1\n",
      "heartbreaker: 1\n",
      "pre: 1\n",
      "commercial: 5\n",
      "fared: 1\n",
      "grinders: 9\n",
      "kruve: 1\n",
      "sifter: 1\n",
      "compare: 1\n",
      "produced: 5\n",
      "timed: 2\n",
      "long: 4\n",
      "powering: 1\n",
      "steamed: 2\n",
      "oat: 2\n",
      "almond: 2\n",
      "brevilles: 2\n",
      "looked: 1\n",
      "visible: 1\n",
      "heard: 1\n",
      "mattered: 1\n",
      "wands: 1\n",
      "maintained: 1\n",
      "smooth: 4\n",
      "sound: 1\n",
      "unpleasant: 1\n",
      "sputtering: 1\n",
      "frothed: 1\n",
      "faster: 2\n",
      "academy: 1\n",
      "retro: 1\n",
      "providing: 1\n",
      "feedback: 1\n",
      "capabilities: 1\n",
      "feel: 5\n",
      "finished: 1\n",
      "lotus: 1\n",
      "art: 2\n",
      "punches: 1\n",
      "far: 1\n",
      "weight: 2\n",
      "duty: 1\n",
      "delight: 1\n",
      "expert: 2\n",
      "aficionados: 1\n",
      "consistency: 3\n",
      "usability: 1\n",
      "outsize: 1\n",
      "sturdy: 1\n",
      "30: 14\n",
      "pounds: 1\n",
      "sits: 2\n",
      "firmly: 1\n",
      "countertop: 3\n",
      "requires: 2\n",
      "intention: 1\n",
      "moved: 3\n",
      "brace: 1\n",
      "locking: 3\n",
      "budge: 1\n",
      "held: 2\n",
      "heft: 1\n",
      "said: 1\n",
      "reminded: 1\n",
      "wall: 11\n",
      "baskets: 7\n",
      "double: 6\n",
      "backflush: 2\n",
      "disc: 1\n",
      "wave: 2\n",
      "acid: 1\n",
      "warm: 2\n",
      "chocolate: 2\n",
      "nutty: 2\n",
      "notes: 7\n",
      "profile: 1\n",
      "depend: 1\n",
      "variables: 1\n",
      "responds: 1\n",
      "slight: 2\n",
      "adjustments: 5\n",
      "set: 5\n",
      "celsius: 1\n",
      "desired: 1\n",
      "helps: 3\n",
      "successive: 1\n",
      "experiment: 2\n",
      "parameter: 1\n",
      "fiddle: 1\n",
      "heats: 1\n",
      "significantly: 1\n",
      "turning: 2\n",
      "couple: 2\n",
      "colder: 1\n",
      "prepped: 1\n",
      "filled: 2\n",
      "jug: 1\n",
      "reached: 1\n",
      "remain: 1\n",
      "ready: 5\n",
      "brew: 4\n",
      "turn: 1\n",
      "timer: 3\n",
      "eco: 1\n",
      "mode: 2\n",
      "shut: 1\n",
      "period: 1\n",
      "seconds: 5\n",
      "screen: 6\n",
      "displays: 3\n",
      "switches: 1\n",
      "led: 1\n",
      "times: 2\n",
      "press: 3\n",
      "tiny: 1\n",
      "automatically: 3\n",
      "starts: 1\n",
      "counting: 1\n",
      "feature: 5\n",
      "saves: 1\n",
      "fumbling: 1\n",
      "phone: 1\n",
      "stopwatch: 1\n",
      "makes: 9\n",
      "difference: 2\n",
      "accurately: 1\n",
      "flaws: 1\n",
      "dealbreakers: 1\n",
      "difficult: 3\n",
      "moving: 1\n",
      "vortex: 1\n",
      "incorporate: 1\n",
      "air: 2\n",
      "obtain: 1\n",
      "achievable: 1\n",
      "thin: 1\n",
      "thinner: 1\n",
      "trickier: 1\n",
      "matter: 1\n",
      "lacks: 3\n",
      "preferably: 1\n",
      "12: 1\n",
      "ounces: 1\n",
      "reduce: 1\n",
      "waste: 1\n",
      "low: 1\n",
      "cost: 4\n",
      "generic: 1\n",
      "assisted: 1\n",
      "convenience: 1\n",
      "hone: 1\n",
      "technique: 1\n",
      "organized: 1\n",
      "menus: 1\n",
      "finesse: 1\n",
      "knowledge: 2\n",
      "appreciate: 1\n",
      "refine: 2\n",
      "results: 5\n",
      "timers: 2\n",
      "custom: 1\n",
      "automation: 3\n",
      "standard: 2\n",
      "gimmick: 1\n",
      "instead: 2\n",
      "sophisticated: 1\n",
      "visually: 1\n",
      "appealing: 1\n",
      "digital: 1\n",
      "menu: 1\n",
      "walks: 2\n",
      "basics: 2\n",
      "troubleshooting: 1\n",
      "factory: 1\n",
      "presets: 2\n",
      "program: 2\n",
      "specifications: 1\n",
      "bambino: 9\n",
      "plus: 11\n",
      "dual-: 1\n",
      "filters: 2\n",
      "pressurized: 1\n",
      "force: 1\n",
      "hole: 1\n",
      "ensures: 1\n",
      "adequate: 2\n",
      "saturation: 1\n",
      "prevent: 1\n",
      "occur: 1\n",
      "inconsistent: 1\n",
      "old: 1\n",
      "uneven: 1\n",
      "tamp: 3\n",
      "body: 1\n",
      "complexity: 2\n",
      "achieve: 3\n",
      "correct: 1\n",
      "spent: 2\n",
      "bulk: 1\n",
      "traditional: 1\n",
      "sure: 3\n",
      "capture: 1\n",
      "lost: 1\n",
      "majority: 1\n",
      "emphasized: 1\n",
      "cocoa: 1\n",
      "y: 1\n",
      "mid: 1\n",
      "tone: 1\n",
      "pleasant: 1\n",
      "tartness: 1\n",
      "depending: 1\n",
      "sang: 1\n",
      "citrus: 2\n",
      "blackberry: 1\n",
      "harder: 5\n",
      "narrower: 1\n",
      "deeper: 1\n",
      "portafilters: 2\n",
      "thicker: 1\n",
      "bed: 1\n",
      "introducing: 1\n",
      "variability: 1\n",
      "accommodate: 1\n",
      "larger: 2\n",
      "fits: 3\n",
      "15: 1\n",
      "grams: 1\n",
      "dry: 1\n",
      "place: 5\n",
      "sensor: 2\n",
      "drip: 8\n",
      "tray: 7\n",
      "slightly: 5\n",
      "alternatively: 1\n",
      "placement: 1\n",
      "handy: 4\n",
      "purge: 2\n",
      "clean: 3\n",
      "scale: 3\n",
      "gives: 3\n",
      "cooler: 3\n",
      "toddler: 1\n",
      "chocolates: 1\n",
      "extra: 2\n",
      "tab: 2\n",
      "pops: 3\n",
      "photo: 6\n",
      "hidden: 1\n",
      "storage: 1\n",
      "box: 4\n",
      "cleaning: 2\n",
      "tools: 1\n",
      "tank: 12\n",
      "holds: 1\n",
      "2: 1\n",
      "liters: 1\n",
      "removable: 4\n",
      "sizes: 4\n",
      "razor: 1\n",
      "tool: 3\n",
      "leveling: 2\n",
      "approached: 1\n",
      "skepticism: 1\n",
      "allowed: 1\n",
      "encore: 4\n",
      "esp: 3\n",
      "burrs: 3\n",
      "owned: 2\n",
      "wheel: 5\n",
      "adjusts: 1\n",
      "settle: 1\n",
      "fine: 5\n",
      "jump: 1\n",
      "large: 2\n",
      "senior: 2\n",
      "editor: 2\n",
      "marguerite: 7\n",
      "preston: 6\n",
      "2019: 3\n",
      "says: 1\n",
      "husband: 1\n",
      "day: 1\n",
      "played: 1\n",
      "fully: 1\n",
      "nerd: 1\n",
      "possible: 1\n",
      "bleary: 1\n",
      "reminders: 1\n",
      "pop: 1\n",
      "descale: 2\n",
      "annoying: 2\n",
      "dismissing: 1\n",
      "probably: 1\n",
      "gets: 3\n",
      "finds: 1\n",
      "tends: 1\n",
      "cool: 3\n",
      "recommends: 2\n",
      "running: 1\n",
      "8: 1\n",
      "attaching: 1\n",
      "messy: 1\n",
      "straight: 1\n",
      "overfill: 1\n",
      "prepared: 2\n",
      "stick: 1\n",
      "shallow: 1\n",
      "bowl: 1\n",
      "catch: 1\n",
      "excess: 1\n",
      "lot: 1\n",
      "caliber: 1\n",
      "skill: 1\n",
      "bodied: 1\n",
      "barebones: 1\n",
      "prioritizes: 1\n",
      "budding: 1\n",
      "geeks: 1\n",
      "satisfy: 1\n",
      "palates: 1\n",
      "sacrificing: 1\n",
      "yielded: 1\n",
      "pricier: 1\n",
      "sampled: 1\n",
      "forth: 1\n",
      "dark: 1\n",
      "bright: 1\n",
      "controller: 2\n",
      "sustain: 1\n",
      "piece: 1\n",
      "interfere: 1\n",
      "luxhaus: 1\n",
      "58: 2\n",
      "mm: 3\n",
      "normcore: 2\n",
      "v4: 1\n",
      "58.5: 1\n",
      "struggle: 1\n",
      "spring: 1\n",
      "loaded: 1\n",
      "self: 1\n",
      "understand: 2\n",
      "lights: 1\n",
      "indicate: 1\n",
      "function: 3\n",
      "likely: 3\n",
      "giving: 1\n",
      "immediately: 1\n",
      "similar: 4\n",
      "accustomed: 1\n",
      "inexpensive: 1\n",
      "inserting: 2\n",
      "rubber: 2\n",
      "tubes: 3\n",
      "refill: 2\n",
      "fills: 2\n",
      "underneath: 3\n",
      "chamber: 1\n",
      "putting: 1\n",
      "hazardous: 1\n",
      "channel: 1\n",
      "inside: 1\n",
      "avoid: 1\n",
      "touching: 1\n",
      "gleaming: 1\n",
      "package: 1\n",
      "suped: 1\n",
      "shape: 1\n",
      "close: 1\n",
      "prior: 1\n",
      "needed: 1\n",
      "ensure: 1\n",
      "setup: 1\n",
      "interactive: 1\n",
      "diagram: 1\n",
      "effectively: 1\n",
      "teaching: 1\n",
      "supplemental: 1\n",
      "clips: 1\n",
      "showing: 1\n",
      "reflects: 1\n",
      "tells: 1\n",
      "based: 2\n",
      "instructions: 1\n",
      "tries: 1\n",
      "look: 3\n",
      "wish: 1\n",
      "disregard: 1\n",
      "guided: 2\n",
      "turned: 1\n",
      "judgement: 1\n",
      "helpful: 1\n",
      "astray: 1\n",
      "limited: 1\n",
      "height: 1\n",
      "change: 2\n",
      "accurate: 2\n",
      "method: 1\n",
      "slowly: 1\n",
      "note: 2\n",
      "pain: 2\n",
      "literal: 1\n",
      "wrist: 1\n",
      "seasoned: 1\n",
      "taken: 1\n",
      "factor: 1\n",
      "outlet: 1\n",
      "stray: 1\n",
      "mess: 1\n",
      "counter: 2\n",
      "magic: 1\n",
      "trick: 1\n",
      "goes: 1\n",
      "focus: 1\n",
      "aspects: 1\n",
      "uses: 1\n",
      "manufacturer: 1\n",
      "45: 2\n",
      "adjustment: 3\n",
      "finest: 3\n",
      "produces: 1\n",
      "identical: 2\n",
      "finer: 2\n",
      "add: 2\n",
      "raises: 1\n",
      "pound: 1\n",
      "reach: 1\n",
      "swivel: 1\n",
      "port: 1\n",
      "fill: 1\n",
      "lifting: 2\n",
      "knock: 4\n",
      "included: 3\n",
      "remove: 1\n",
      "inclusion: 1\n",
      "cleanup: 1\n",
      "quick: 1\n",
      "wider: 1\n",
      "shallower: 1\n",
      "cold: 4\n",
      "pleasantly: 1\n",
      "surprised: 1\n",
      "20: 3\n",
      "came: 1\n",
      "noticeably: 2\n",
      "watery: 1\n",
      "cooled: 1\n",
      "cooking: 1\n",
      "baking: 1\n",
      "mug: 1\n",
      "lukewarm: 1\n",
      "95: 1\n",
      "americano: 2\n",
      "ish: 1\n",
      "ice: 1\n",
      "job: 1\n",
      "functions: 1\n",
      "textures: 1\n",
      "temperatures: 1\n",
      "soy: 1\n",
      "gauge: 1\n",
      "tip: 1\n",
      "glossy: 1\n",
      "pour: 2\n",
      "responsibility: 1\n",
      "tag: 1\n",
      "normally: 1\n",
      "balk: 1\n",
      "cheated: 1\n",
      "utterly: 1\n",
      "undemanding: 1\n",
      "dollars: 1\n",
      "standalone: 1\n",
      "mental: 1\n",
      "energy: 1\n",
      "willing: 2\n",
      "pay: 1\n",
      "lelit: 2\n",
      "mara: 1\n",
      "x: 1\n",
      "sports: 1\n",
      "car: 1\n",
      "tampers: 1\n",
      "mechanism: 1\n",
      "curved: 1\n",
      "spouted: 1\n",
      "watch: 1\n",
      "slide: 1\n",
      "sumptuous: 1\n",
      "hint: 1\n",
      "smokiness: 1\n",
      "lighter: 4\n",
      "florals: 1\n",
      "honey: 1\n",
      "aside: 1\n",
      "aesthetics: 1\n",
      "costs: 3\n",
      "600: 1\n",
      "lower: 1\n",
      "similarly: 1\n",
      "reprogram: 1\n",
      "crucial: 1\n",
      "youâll: 1\n",
      "burr: 3\n",
      "specifically: 1\n",
      "tend: 2\n",
      "increments: 1\n",
      "absolutely: 1\n",
      "try: 1\n",
      "precise: 2\n",
      "tuning: 2\n",
      "keen: 1\n",
      "spend: 2\n",
      "money: 1\n",
      "suffice: 1\n",
      "currently: 1\n",
      "methods: 1\n",
      "french: 1\n",
      "combination: 1\n",
      "constrain: 1\n",
      "type: 1\n",
      "elegantly: 1\n",
      "comparison: 1\n",
      "conical: 2\n",
      "31: 1\n",
      "cleanly: 2\n",
      "refined: 1\n",
      "grade: 1\n",
      "precision: 3\n",
      "hundredth: 1\n",
      "second: 3\n",
      "given: 1\n",
      "drawback: 1\n",
      "detachable: 1\n",
      "arms: 2\n",
      "kind: 2\n",
      "securely: 2\n",
      "overall: 1\n",
      "400: 1\n",
      "roasts: 1\n",
      "origin: 1\n",
      "coffees: 1\n",
      "distinctive: 1\n",
      "fruitier: 1\n",
      "sensitive: 1\n",
      "ideal: 1\n",
      "functionality: 1\n",
      "stepless: 2\n",
      "smaller: 2\n",
      "tweaks: 1\n",
      "total: 1\n",
      "individual: 1\n",
      "seamless: 1\n",
      "duration: 1\n",
      "importantly: 1\n",
      "away: 1\n",
      "sufficient: 1\n",
      "intermediate: 1\n",
      "expressive: 1\n",
      "par: 1\n",
      "anna: 1\n",
      "pl41tem: 1\n",
      "ergonomic: 1\n",
      "hitting: 1\n",
      "different: 1\n",
      "lacked: 1\n",
      "struggled: 2\n",
      "textured: 1\n",
      "ascaso: 1\n",
      "dream: 1\n",
      "cute: 1\n",
      "nearly: 1\n",
      "bad: 1\n",
      "fruity: 1\n",
      "fragrant: 1\n",
      "flat: 1\n",
      "leakage: 1\n",
      "escapes: 1\n",
      "slides: 1\n",
      "seams: 1\n",
      "fewer: 1\n",
      "customization: 1\n",
      "simpler: 1\n",
      "compact: 1\n",
      "infuser: 3\n",
      "creamy: 1\n",
      "express: 1\n",
      "basically: 1\n",
      "missing: 1\n",
      "selling: 1\n",
      "opting: 1\n",
      "affords: 1\n",
      "duo: 1\n",
      "temp: 1\n",
      "buttons: 1\n",
      "stop: 1\n",
      "coarsely: 1\n",
      "resulting: 1\n",
      "duller: 1\n",
      "recurring: 1\n",
      "issues: 1\n",
      "solis: 1\n",
      "perfetta: 1\n",
      "despite: 1\n",
      "featuring: 1\n",
      "hottest: 1\n",
      "cut: 2\n",
      "frequently: 1\n",
      "leaked: 2\n",
      "deâlonghi: 1\n",
      "dedica: 1\n",
      "tended: 2\n",
      "muddy: 1\n",
      "awkward: 1\n",
      "cappuccino: 1\n",
      "switch: 1\n",
      "bumped: 1\n",
      "wrong: 1\n",
      "carezza: 1\n",
      "deluxe: 1\n",
      "oriented: 1\n",
      "simply: 1\n",
      "bubbly: 1\n",
      "generally: 1\n",
      "unremarkable: 1\n",
      "sour: 1\n",
      "occasionally: 1\n",
      "rancilio: 1\n",
      "silvia: 2\n",
      "900: 1\n",
      "paying: 1\n",
      "regulated: 1\n",
      "article: 1\n",
      "edited: 1\n",
      "marilyn: 1\n",
      "ong: 1\n",
      "alison: 1\n",
      "nowak: 1\n",
      "george: 1\n",
      "howell: 1\n",
      "email: 4\n",
      "interview: 5\n",
      "january: 2\n",
      "24: 1\n",
      "phil: 1\n",
      "mcknight: 1\n",
      "global: 1\n",
      "manager: 1\n",
      "22: 1\n",
      "matthew: 1\n",
      "davis: 1\n",
      "toast: 1\n",
      "august: 1\n",
      "2020: 1\n",
      "owners: 1\n",
      "april: 1\n",
      "owner: 1\n",
      "person: 1\n",
      "september: 1\n",
      "1: 1\n",
      "meet: 1\n",
      "team: 1\n",
      "previously: 1\n",
      "artisanal: 1\n",
      "cheesemaker: 1\n",
      "farm: 1\n",
      "vermont: 1\n",
      "reading: 1\n",
      "handpresso: 2\n",
      "nanopresso: 2\n",
      "handheld: 1\n",
      "makers: 1\n",
      "nightmare: 1\n",
      "insist: 1\n",
      "brian: 1\n",
      "lam: 1\n",
      "know: 1\n",
      "aeropress: 3\n",
      "portable: 1\n",
      "frills: 1\n",
      "terrific: 1\n",
      "daniel: 1\n",
      "varghese: 1\n",
      "alex: 1\n",
      "arpaia: 1\n",
      "maker: 1\n",
      "travel: 1\n",
      "companion: 1\n",
      "cups: 1\n",
      "laura: 1\n",
      "motley: 1\n",
      "tyler: 1\n",
      "wells: 1\n",
      "lynch: 1\n",
      "dependable: 1\n",
      "cook: 1\n",
      "dishes: 1\n",
      "edit: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def analyze_frequent_tokens(html_file):\n",
    "    # Read the HTML file and extract text\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and get the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # Create a list of tokens (converted to lowercase) and filter out stop words, punctuation, and spaces\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "    # Use Counter to count the frequency of each token\n",
    "    token_frequencies = Counter(tokens)\n",
    "\n",
    "    # Get the 5 most common tokens\n",
    "    most_common_tokens = token_frequencies.most_common(5)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Top 5 Most Frequent Tokens:\")\n",
    "    for token, freq in most_common_tokens:\n",
    "        print(f\"{token}: {freq}\")\n",
    "\n",
    "    print(\"\\nToken Frequencies:\")\n",
    "    for token, freq in token_frequencies.items():\n",
    "        print(f\"{token}: {freq}\")\n",
    "\n",
    "# Example: Analyze frequent tokens for the saved article HTML\n",
    "html_file = 'cleaned_coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "analyze_frequent_tokens(html_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Frequent Lemmas:\n",
      "machine: 136\n",
      "espresso: 107\n",
      "shot: 98\n",
      "breville: 55\n",
      "barista: 53\n",
      "\n",
      "Lemma Frequencies:\n",
      "4: 1\n",
      "best: 7\n",
      "home: 17\n",
      "espresso: 107\n",
      "machines: 1\n",
      "2024: 5\n",
      "|: 1\n",
      "reviews: 1\n",
      "wirecutter: 31\n",
      "advertisement: 18\n",
      "skip: 10\n",
      "content: 1\n",
      "help: 11\n",
      "decide: 1\n",
      "research: 2\n",
      "log: 1\n",
      "account: 1\n",
      "independently: 1\n",
      "review: 4\n",
      "recommend: 13\n",
      "buy: 11\n",
      "link: 1\n",
      "earn: 1\n",
      "commission: 1\n",
      "learn: 8\n",
      "deal: 7\n",
      "breville: 55\n",
      "barista: 53\n",
      "touch: 46\n",
      "machine: 136\n",
      "$: 40\n",
      "693: 5\n",
      "27: 2\n",
      "kitchen: 7\n",
      "coffee: 49\n",
      "update: 3\n",
      "november: 2\n",
      "25: 1\n",
      "save: 6\n",
      "connie: 19\n",
      "park: 19\n",
      "nyt: 23\n",
      "justin: 3\n",
      "vassallo: 2\n",
      "ciara: 4\n",
      "murray: 4\n",
      "jordan: 4\n",
      "fyi: 1\n",
      "new: 8\n",
      "round: 3\n",
      "testing: 7\n",
      "profitec: 26\n",
      "pick: 43\n",
      "oracle: 29\n",
      "jet: 29\n",
      "upgrade: 9\n",
      "gaggia: 25\n",
      "classic: 21\n",
      "evo: 11\n",
      "pro: 24\n",
      "make: 27\n",
      "cafÃ©: 11\n",
      "quality: 13\n",
      "drink: 23\n",
      "long: 9\n",
      "reserve: 1\n",
      "hobbyist: 2\n",
      "easy: 13\n",
      "beginner: 15\n",
      "start: 9\n",
      "little: 14\n",
      "practice: 8\n",
      "latte: 11\n",
      "cortado: 3\n",
      "rival: 1\n",
      "upscale: 1\n",
      "change: 5\n",
      "pajama: 1\n",
      "test: 16\n",
      "dozen: 3\n",
      "think: 10\n",
      "good: 26\n",
      "option: 10\n",
      "skilled: 3\n",
      "enthusiast: 1\n",
      "alike: 2\n",
      "powerful: 14\n",
      "use: 22\n",
      "capable: 6\n",
      "yield: 7\n",
      "consistent: 19\n",
      "rich: 1\n",
      "shot: 98\n",
      "robust: 2\n",
      "flavor: 13\n",
      "profile: 3\n",
      "pull: 44\n",
      "deep: 4\n",
      "flavorful: 5\n",
      "combine: 3\n",
      "capability: 4\n",
      "higher: 3\n",
      "price: 11\n",
      "basic: 7\n",
      "interface: 3\n",
      "simple: 7\n",
      "build: 22\n",
      "power: 8\n",
      "appeal: 3\n",
      "buying: 14\n",
      "options: 14\n",
      "1,059: 6\n",
      "love: 6\n",
      "clive: 3\n",
      "runner: 6\n",
      "automate: 5\n",
      "sleek: 5\n",
      "great: 18\n",
      "programming: 4\n",
      "solid: 7\n",
      "grinder: 51\n",
      "let: 7\n",
      "variety: 4\n",
      "learning: 4\n",
      "curve: 5\n",
      "950: 4\n",
      "amazon: 9\n",
      "stainless: 5\n",
      "steel: 5\n",
      "899: 4\n",
      "walmart: 4\n",
      "1,000: 4\n",
      "budget: 8\n",
      "nuanced: 7\n",
      "entry: 5\n",
      "level: 11\n",
      "affordable: 3\n",
      "surprisingly: 4\n",
      "complex: 7\n",
      "need: 20\n",
      "separate: 3\n",
      "tamper: 23\n",
      "froth: 25\n",
      "milk: 46\n",
      "bit: 10\n",
      "tricky: 9\n",
      "microfoam: 10\n",
      "manageable: 3\n",
      "457: 3\n",
      "549: 3\n",
      "sur: 5\n",
      "la: 5\n",
      "table: 5\n",
      "ultimate: 2\n",
      "semi: 5\n",
      "automatic: 20\n",
      "grind: 44\n",
      "dose: 12\n",
      "tamp: 11\n",
      "guide: 11\n",
      "entire: 5\n",
      "process: 10\n",
      "2,000: 6\n",
      "heavy: 3\n",
      "impressive: 4\n",
      "range: 6\n",
      "depth: 5\n",
      "dial: 13\n",
      "highlight: 1\n",
      "particular: 2\n",
      "characteristic: 1\n",
      "bean: 11\n",
      "straightforward: 3\n",
      "attractive: 1\n",
      "design: 10\n",
      "comfortable: 6\n",
      "portafilter: 26\n",
      "lock: 4\n",
      "group: 4\n",
      "head: 5\n",
      "easily: 5\n",
      "steam: 44\n",
      "wand: 29\n",
      "angle: 5\n",
      "wide: 2\n",
      "height: 2\n",
      "direction: 1\n",
      "especially: 5\n",
      "take: 18\n",
      "beautiful: 3\n",
      "silky: 4\n",
      "foam: 3\n",
      "seriously: 1\n",
      "interested: 2\n",
      "making: 3\n",
      "serve: 2\n",
      "novice: 3\n",
      "offer: 8\n",
      "lot: 2\n",
      "guidance: 3\n",
      "form: 1\n",
      "touchscreen: 8\n",
      "control: 12\n",
      "center: 3\n",
      "step: 13\n",
      "tutorial: 3\n",
      "multiple: 3\n",
      "program: 7\n",
      "include: 7\n",
      "advanced: 1\n",
      "allow: 25\n",
      "manual: 7\n",
      "operation: 1\n",
      "experienced: 1\n",
      "user: 2\n",
      "people: 4\n",
      "want: 17\n",
      "creative: 1\n",
      "adjustable: 1\n",
      "auto: 4\n",
      "setting: 23\n",
      "create: 5\n",
      "choice: 1\n",
      "dive: 1\n",
      "right: 6\n",
      "decent: 6\n",
      "hand: 6\n",
      "experience: 6\n",
      "version: 2\n",
      "popular: 1\n",
      "decade: 1\n",
      "thank: 1\n",
      "approachable: 1\n",
      "ability: 4\n",
      "produce: 12\n",
      "worthy: 1\n",
      "somewhat: 1\n",
      "improved: 1\n",
      "compare: 3\n",
      "weak: 3\n",
      "velvety: 2\n",
      "texture: 5\n",
      "time: 16\n",
      "master: 2\n",
      "able: 6\n",
      "nuance: 3\n",
      "taste: 7\n",
      "high: 5\n",
      "spare: 1\n",
      "strongly: 1\n",
      "replace: 2\n",
      "flimsy: 4\n",
      "plastic: 4\n",
      "come: 17\n",
      "work: 6\n",
      "unlike: 7\n",
      "super: 4\n",
      "teach: 2\n",
      "well: 13\n",
      "walk: 5\n",
      "detect: 1\n",
      "small: 15\n",
      "require: 5\n",
      "like: 19\n",
      "choose: 5\n",
      "non: 5\n",
      "dairy: 3\n",
      "housegu: 1\n",
      "end: 1\n",
      "pretty: 2\n",
      "trust: 2\n",
      "worth: 3\n",
      "consider: 7\n",
      "competition: 2\n",
      "source: 2\n",
      "associate: 2\n",
      "staff: 2\n",
      "writer: 3\n",
      "cover: 4\n",
      "gear: 2\n",
      "nespresso: 1\n",
      "frother: 1\n",
      "familiar: 2\n",
      "expect: 2\n",
      "preparation: 3\n",
      "class: 1\n",
      "project: 8\n",
      "ny: 7\n",
      "sensory: 1\n",
      "evaluation: 1\n",
      "course: 4\n",
      "vasallo: 1\n",
      "previous: 2\n",
      "lead: 4\n",
      "10: 2\n",
      "year: 2\n",
      "volume: 3\n",
      "shop: 1\n",
      "york: 2\n",
      "city: 3\n",
      "boston: 1\n",
      "read: 2\n",
      "article: 2\n",
      "blog: 1\n",
      "post: 1\n",
      "expert: 4\n",
      "watch: 2\n",
      "product: 4\n",
      "demo: 1\n",
      "video: 3\n",
      "site: 1\n",
      "seattle: 1\n",
      "sell: 1\n",
      "equipment: 1\n",
      "2021: 2\n",
      "interview: 7\n",
      "chisum: 2\n",
      "ngai: 2\n",
      "kaleena: 6\n",
      "teoh: 6\n",
      "past: 1\n",
      "beverage: 4\n",
      "category: 3\n",
      "bring: 7\n",
      "lineup: 1\n",
      "founder: 2\n",
      "try: 6\n",
      "provide: 6\n",
      "professional: 1\n",
      "opinion: 1\n",
      "journalist: 1\n",
      "complete: 1\n",
      "editorial: 3\n",
      "independence: 1\n",
      "aware: 1\n",
      "business: 2\n",
      "implication: 1\n",
      "recommendation: 3\n",
      "standard: 3\n",
      "commitment: 1\n",
      "obsess: 1\n",
      "investment: 2\n",
      "quickly: 9\n",
      "prove: 1\n",
      "dedicated: 2\n",
      "drinker: 3\n",
      "real: 1\n",
      "bet: 1\n",
      "way: 4\n",
      "true: 2\n",
      "quick: 2\n",
      "cup: 5\n",
      "morning: 2\n",
      "heat: 9\n",
      "kettle: 1\n",
      "boil: 1\n",
      "dosing: 2\n",
      "tamping: 3\n",
      "modicum: 1\n",
      "effort: 2\n",
      "minute: 8\n",
      "sit: 3\n",
      "hot: 8\n",
      "crema: 1\n",
      "top: 1\n",
      "prioritize: 2\n",
      "deliver: 2\n",
      "delicious: 2\n",
      "tinkering: 1\n",
      "find: 10\n",
      "happy: 1\n",
      "style: 1\n",
      "cappuccino: 3\n",
      "recreate: 1\n",
      "favorite: 2\n",
      "milky: 1\n",
      "comfort: 1\n",
      "tailor: 1\n",
      "liking: 1\n",
      "blend: 6\n",
      "adjust: 10\n",
      "brewing: 7\n",
      "factor: 3\n",
      "certain: 2\n",
      "exact: 2\n",
      "sound: 2\n",
      "intimidate: 1\n",
      "hold: 11\n",
      "standout: 1\n",
      "push: 2\n",
      "button: 7\n",
      "sarah: 9\n",
      "kobos: 9\n",
      "cheap: 2\n",
      "toylike: 1\n",
      "hulk: 1\n",
      "cost: 9\n",
      "thousand: 1\n",
      "2016: 2\n",
      "model: 7\n",
      "300: 2\n",
      "sweet: 2\n",
      "spot: 1\n",
      "obsessive: 1\n",
      "enjoy: 1\n",
      "value: 3\n",
      "feature: 14\n",
      "baseline: 1\n",
      "force: 2\n",
      "water: 21\n",
      "finely: 4\n",
      "pressure: 10\n",
      "one: 3\n",
      "temperature: 15\n",
      "195: 1\n",
      "205: 1\n",
      "degree: 2\n",
      "fahrenheit: 3\n",
      "apply: 1\n",
      "flow: 3\n",
      "ground: 10\n",
      "evenly: 4\n",
      "balanced: 8\n",
      "extraction: 11\n",
      "ease: 4\n",
      "transition: 5\n",
      "luxe: 2\n",
      "part: 1\n",
      "accessory: 4\n",
      "following: 1\n",
      "criterion: 1\n",
      "narrow: 2\n",
      "pump: 4\n",
      "size: 16\n",
      "ratio: 1\n",
      "balance: 5\n",
      "prefer: 3\n",
      "lever: 5\n",
      "hard: 7\n",
      "over-: 2\n",
      "extract: 7\n",
      "jura: 1\n",
      "measure: 1\n",
      "room: 2\n",
      "experimentation: 3\n",
      "pricey: 2\n",
      "left: 1\n",
      "big: 3\n",
      "single: 18\n",
      "boiler: 13\n",
      "mean: 1\n",
      "wait: 4\n",
      "technology: 3\n",
      "advance: 1\n",
      "dual: 5\n",
      "simultaneously: 1\n",
      "expensive: 4\n",
      "will: 6\n",
      "entail: 1\n",
      "multitaske: 1\n",
      "usually: 1\n",
      "necessary: 2\n",
      "fast: 4\n",
      "heater: 3\n",
      "speedy: 1\n",
      "fun: 1\n",
      "rhythm: 1\n",
      "promise: 1\n",
      "daily: 1\n",
      "ritual: 1\n",
      "pid: 7\n",
      "proportional: 1\n",
      "integral: 2\n",
      "derivative: 1\n",
      "controller: 3\n",
      "regulate: 3\n",
      "thermojet: 2\n",
      "barely: 3\n",
      "finish: 3\n",
      "strong: 4\n",
      "properly: 6\n",
      "pack: 1\n",
      "free: 1\n",
      "bubble: 2\n",
      "key: 2\n",
      "component: 1\n",
      "distribute: 1\n",
      "compact: 2\n",
      "order: 2\n",
      "path: 1\n",
      "resistance: 1\n",
      "point: 3\n",
      "puck: 7\n",
      "call: 1\n",
      "channeling: 1\n",
      "cause: 2\n",
      "unbalanced: 2\n",
      "width: 2\n",
      "filter: 14\n",
      "basket: 17\n",
      "handle: 2\n",
      "base: 5\n",
      "aid: 1\n",
      "exert: 1\n",
      "prevent: 2\n",
      "friendly: 2\n",
      "frothing: 5\n",
      "effective: 3\n",
      "pitcher: 12\n",
      "thing: 3\n",
      "distinguish: 1\n",
      "excellent: 2\n",
      "ultimately: 1\n",
      "achieve: 4\n",
      "manually: 6\n",
      "online: 1\n",
      "flaw: 2\n",
      "programmable: 2\n",
      "programmed: 1\n",
      "vary: 1\n",
      "stop: 3\n",
      "alongside: 1\n",
      "tweaking: 1\n",
      "nice: 3\n",
      "and/or: 1\n",
      "timing: 1\n",
      "streamline: 1\n",
      "routine: 1\n",
      "get: 8\n",
      "late: 1\n",
      "baratza: 13\n",
      "sette: 19\n",
      "270: 10\n",
      "ninth: 1\n",
      "street: 1\n",
      "alphabet: 1\n",
      "blue: 1\n",
      "bottle: 1\n",
      "hayes: 1\n",
      "valley: 1\n",
      "grumpy: 1\n",
      "heartbreaker: 1\n",
      "pre: 1\n",
      "commercial: 5\n",
      "fare: 1\n",
      "kruve: 1\n",
      "sifter: 1\n",
      "powering: 1\n",
      "steaming: 2\n",
      "oat: 2\n",
      "almond: 2\n",
      "brevilles: 2\n",
      "look: 4\n",
      "visible: 1\n",
      "heard: 1\n",
      "matter: 2\n",
      "maintain: 1\n",
      "smooth: 4\n",
      "unpleasant: 1\n",
      "sputtering: 1\n",
      "academy: 1\n",
      "retro: 1\n",
      "feedback: 1\n",
      "feel: 5\n",
      "lotus: 1\n",
      "art: 2\n",
      "punch: 1\n",
      "far: 1\n",
      "weight: 2\n",
      "duty: 1\n",
      "delight: 1\n",
      "aficionado: 1\n",
      "consistency: 3\n",
      "usability: 1\n",
      "outsize: 1\n",
      "sturdy: 1\n",
      "30: 14\n",
      "pound: 2\n",
      "firmly: 1\n",
      "countertop: 3\n",
      "intention: 1\n",
      "move: 4\n",
      "brace: 1\n",
      "budge: 1\n",
      "heft: 1\n",
      "say: 2\n",
      "remind: 1\n",
      "wall: 11\n",
      "double: 6\n",
      "backflush: 2\n",
      "disc: 1\n",
      "wave: 2\n",
      "acid: 1\n",
      "warm: 2\n",
      "chocolate: 3\n",
      "nutty: 2\n",
      "note: 9\n",
      "depend: 2\n",
      "variable: 1\n",
      "respond: 1\n",
      "slight: 2\n",
      "adjustment: 8\n",
      "set: 6\n",
      "celsius: 1\n",
      "desire: 1\n",
      "successive: 1\n",
      "experiment: 2\n",
      "parameter: 1\n",
      "fiddle: 1\n",
      "significantly: 1\n",
      "turn: 4\n",
      "couple: 2\n",
      "colder: 1\n",
      "preppe: 1\n",
      "fill: 5\n",
      "jug: 1\n",
      "reach: 2\n",
      "remain: 1\n",
      "ready: 5\n",
      "brew: 5\n",
      "timer: 5\n",
      "eco: 1\n",
      "mode: 2\n",
      "shut: 1\n",
      "period: 1\n",
      "second: 8\n",
      "screen: 6\n",
      "display: 3\n",
      "switch: 2\n",
      "led: 1\n",
      "press: 3\n",
      "tiny: 1\n",
      "automatically: 3\n",
      "count: 1\n",
      "fumble: 1\n",
      "phone: 1\n",
      "stopwatch: 1\n",
      "difference: 2\n",
      "accurately: 1\n",
      "dealbreaker: 1\n",
      "difficult: 3\n",
      "vortex: 1\n",
      "incorporate: 1\n",
      "air: 2\n",
      "obtain: 1\n",
      "achievable: 1\n",
      "thin: 2\n",
      "getting: 1\n",
      "lack: 4\n",
      "preferably: 1\n",
      "12: 1\n",
      "ounce: 1\n",
      "reduce: 1\n",
      "waste: 1\n",
      "low: 2\n",
      "generic: 1\n",
      "assist: 1\n",
      "convenience: 1\n",
      "hone: 1\n",
      "technique: 1\n",
      "organize: 1\n",
      "menu: 2\n",
      "finesse: 1\n",
      "knowledge: 2\n",
      "appreciate: 1\n",
      "refine: 3\n",
      "result: 6\n",
      "custom: 1\n",
      "automation: 3\n",
      "gimmick: 1\n",
      "instead: 2\n",
      "sophisticated: 1\n",
      "visually: 1\n",
      "appealing: 1\n",
      "digital: 1\n",
      "troubleshooting: 1\n",
      "factory: 1\n",
      "preset: 2\n",
      "specification: 1\n",
      "bambino: 9\n",
      "plus: 11\n",
      "dual-: 1\n",
      "pressurized: 1\n",
      "hole: 1\n",
      "ensure: 2\n",
      "adequate: 2\n",
      "saturation: 1\n",
      "occur: 1\n",
      "inconsistent: 1\n",
      "old: 1\n",
      "uneven: 1\n",
      "body: 1\n",
      "complexity: 2\n",
      "correct: 1\n",
      "spend: 4\n",
      "bulk: 1\n",
      "traditional: 1\n",
      "sure: 3\n",
      "capture: 1\n",
      "lose: 1\n",
      "majority: 1\n",
      "emphasize: 1\n",
      "cocoa: 1\n",
      "y: 1\n",
      "mid: 1\n",
      "tone: 1\n",
      "pleasant: 1\n",
      "tartness: 1\n",
      "sing: 1\n",
      "citrus: 2\n",
      "blackberry: 1\n",
      "thick: 1\n",
      "bed: 1\n",
      "introduce: 1\n",
      "variability: 1\n",
      "accommodate: 1\n",
      "large: 4\n",
      "fit: 3\n",
      "15: 1\n",
      "gram: 1\n",
      "dry: 1\n",
      "place: 5\n",
      "sensor: 2\n",
      "drip: 8\n",
      "tray: 7\n",
      "slightly: 5\n",
      "alternatively: 1\n",
      "placement: 1\n",
      "handy: 4\n",
      "purge: 2\n",
      "clean: 4\n",
      "scale: 3\n",
      "give: 5\n",
      "cool: 7\n",
      "toddler: 1\n",
      "extra: 2\n",
      "tab: 2\n",
      "pop: 4\n",
      "photo: 6\n",
      "hide: 1\n",
      "storage: 1\n",
      "box: 4\n",
      "cleaning: 1\n",
      "tool: 4\n",
      "tank: 12\n",
      "2: 1\n",
      "liter: 1\n",
      "removable: 4\n",
      "razor: 1\n",
      "approach: 1\n",
      "skepticism: 1\n",
      "encore: 4\n",
      "esp: 3\n",
      "burr: 6\n",
      "own: 2\n",
      "wheel: 5\n",
      "settle: 1\n",
      "fine: 10\n",
      "jump: 1\n",
      "senior: 2\n",
      "editor: 2\n",
      "marguerite: 7\n",
      "preston: 6\n",
      "2019: 3\n",
      "husband: 1\n",
      "day: 1\n",
      "play: 1\n",
      "fully: 1\n",
      "nerd: 1\n",
      "possible: 1\n",
      "bleary: 1\n",
      "reminder: 1\n",
      "descale: 2\n",
      "annoying: 2\n",
      "dismiss: 1\n",
      "probably: 1\n",
      "tend: 5\n",
      "run: 1\n",
      "8: 1\n",
      "attach: 1\n",
      "messy: 1\n",
      "straight: 1\n",
      "overfill: 1\n",
      "prepare: 1\n",
      "stick: 1\n",
      "shallow: 2\n",
      "bowl: 1\n",
      "catch: 1\n",
      "excess: 1\n",
      "prepared: 1\n",
      "caliber: 1\n",
      "skill: 1\n",
      "bodied: 1\n",
      "barebone: 1\n",
      "bud: 1\n",
      "geek: 1\n",
      "satisfy: 1\n",
      "palate: 1\n",
      "sacrifice: 1\n",
      "pricy: 1\n",
      "sample: 1\n",
      "forth: 1\n",
      "dark: 1\n",
      "bright: 1\n",
      "sustain: 1\n",
      "piece: 1\n",
      "interfere: 1\n",
      "luxhaus: 1\n",
      "58: 2\n",
      "mm: 3\n",
      "normcore: 2\n",
      "v4: 1\n",
      "58.5: 1\n",
      "struggle: 3\n",
      "spring: 1\n",
      "load: 1\n",
      "self: 1\n",
      "understand: 2\n",
      "light: 5\n",
      "indicate: 1\n",
      "function: 4\n",
      "likely: 3\n",
      "immediately: 1\n",
      "similar: 4\n",
      "accustomed: 1\n",
      "inexpensive: 1\n",
      "insert: 2\n",
      "rubber: 2\n",
      "tube: 3\n",
      "refill: 2\n",
      "underneath: 3\n",
      "chamber: 1\n",
      "put: 1\n",
      "hazardous: 1\n",
      "channel: 1\n",
      "inside: 1\n",
      "avoid: 1\n",
      "gleam: 1\n",
      "package: 1\n",
      "sup: 1\n",
      "shape: 1\n",
      "close: 1\n",
      "prior: 1\n",
      "setup: 1\n",
      "interactive: 1\n",
      "diagram: 1\n",
      "effectively: 1\n",
      "supplemental: 1\n",
      "clip: 1\n",
      "show: 1\n",
      "reflect: 1\n",
      "tell: 1\n",
      "follow: 1\n",
      "instruction: 1\n",
      "wish: 1\n",
      "disregard: 1\n",
      "dialing: 4\n",
      "judgement: 1\n",
      "helpful: 1\n",
      "astray: 1\n",
      "limited: 1\n",
      "accurate: 2\n",
      "method: 2\n",
      "slowly: 1\n",
      "pain: 2\n",
      "literal: 1\n",
      "wrist: 1\n",
      "seasoned: 1\n",
      "outlet: 1\n",
      "stray: 1\n",
      "mess: 1\n",
      "counter: 2\n",
      "magic: 1\n",
      "trick: 1\n",
      "go: 1\n",
      "focus: 1\n",
      "aspect: 1\n",
      "manufacturer: 1\n",
      "45: 2\n",
      "identical: 2\n",
      "add: 2\n",
      "raise: 1\n",
      "swivel: 1\n",
      "port: 1\n",
      "lift: 1\n",
      "knock: 4\n",
      "lifting: 1\n",
      "remove: 1\n",
      "inclusion: 1\n",
      "cleanup: 1\n",
      "cold: 4\n",
      "pleasantly: 1\n",
      "surprise: 1\n",
      "20: 3\n",
      "noticeably: 2\n",
      "watery: 1\n",
      "cook: 2\n",
      "bake: 1\n",
      "mug: 1\n",
      "lukewarm: 1\n",
      "95: 1\n",
      "americano: 2\n",
      "leave: 1\n",
      "ish: 1\n",
      "ice: 1\n",
      "job: 1\n",
      "soy: 1\n",
      "gauge: 1\n",
      "tip: 1\n",
      "glossy: 1\n",
      "pour: 2\n",
      "responsibility: 1\n",
      "tag: 1\n",
      "normally: 1\n",
      "balk: 1\n",
      "cheat: 1\n",
      "utterly: 1\n",
      "undemande: 1\n",
      "dollar: 1\n",
      "standalone: 1\n",
      "mental: 1\n",
      "energy: 1\n",
      "willing: 2\n",
      "pay: 2\n",
      "lelit: 2\n",
      "mara: 1\n",
      "x: 1\n",
      "sport: 1\n",
      "car: 1\n",
      "mechanism: 1\n",
      "spout: 1\n",
      "slide: 2\n",
      "sumptuous: 1\n",
      "hint: 1\n",
      "smokiness: 1\n",
      "floral: 1\n",
      "honey: 1\n",
      "aside: 1\n",
      "aesthetic: 1\n",
      "600: 1\n",
      "similarly: 1\n",
      "reprogram: 1\n",
      "crucial: 1\n",
      "youâll: 1\n",
      "specifically: 1\n",
      "increment: 1\n",
      "absolutely: 1\n",
      "precise: 2\n",
      "tuning: 1\n",
      "keen: 1\n",
      "money: 1\n",
      "suffice: 1\n",
      "currently: 1\n",
      "french: 1\n",
      "combination: 1\n",
      "constrain: 1\n",
      "type: 1\n",
      "elegantly: 1\n",
      "comparison: 1\n",
      "tune: 1\n",
      "conical: 2\n",
      "31: 1\n",
      "cleanly: 2\n",
      "grade: 1\n",
      "precision: 3\n",
      "hundredth: 1\n",
      "drawback: 1\n",
      "detachable: 1\n",
      "arm: 2\n",
      "kind: 2\n",
      "securely: 2\n",
      "overall: 1\n",
      "400: 1\n",
      "roast: 1\n",
      "origin: 1\n",
      "distinctive: 1\n",
      "fruity: 2\n",
      "sensitive: 1\n",
      "ideal: 1\n",
      "functionality: 1\n",
      "stepless: 2\n",
      "tweak: 1\n",
      "total: 1\n",
      "individual: 1\n",
      "seamless: 1\n",
      "duration: 1\n",
      "importantly: 1\n",
      "away: 1\n",
      "sufficient: 1\n",
      "intermediate: 1\n",
      "expressive: 1\n",
      "par: 1\n",
      "anna: 1\n",
      "pl41tem: 1\n",
      "ergonomic: 1\n",
      "hit: 1\n",
      "different: 1\n",
      "ascaso: 1\n",
      "dream: 1\n",
      "cute: 1\n",
      "nearly: 1\n",
      "bad: 1\n",
      "fragrant: 1\n",
      "flat: 1\n",
      "leakage: 1\n",
      "escape: 1\n",
      "seam: 1\n",
      "few: 1\n",
      "customization: 1\n",
      "infuser: 3\n",
      "creamy: 1\n",
      "express: 1\n",
      "basically: 1\n",
      "miss: 1\n",
      "selling: 1\n",
      "opt: 1\n",
      "afford: 1\n",
      "duo: 1\n",
      "temp: 1\n",
      "coarsely: 1\n",
      "duller: 1\n",
      "recur: 1\n",
      "issue: 1\n",
      "solis: 1\n",
      "perfetta: 1\n",
      "despite: 1\n",
      "cut: 2\n",
      "frequently: 1\n",
      "leak: 2\n",
      "deâlonghi: 1\n",
      "dedica: 1\n",
      "muddy: 1\n",
      "awkward: 1\n",
      "bump: 1\n",
      "wrong: 1\n",
      "carezza: 1\n",
      "deluxe: 1\n",
      "orient: 1\n",
      "simply: 1\n",
      "bubbly: 1\n",
      "generally: 1\n",
      "unremarkable: 1\n",
      "sour: 1\n",
      "occasionally: 1\n",
      "rancilio: 1\n",
      "silvia: 2\n",
      "900: 1\n",
      "edit: 2\n",
      "marilyn: 1\n",
      "ong: 1\n",
      "alison: 1\n",
      "nowak: 1\n",
      "george: 1\n",
      "howell: 1\n",
      "email: 4\n",
      "january: 2\n",
      "24: 1\n",
      "phil: 1\n",
      "mcknight: 1\n",
      "global: 1\n",
      "manager: 1\n",
      "22: 1\n",
      "matthew: 1\n",
      "davis: 1\n",
      "toast: 1\n",
      "august: 1\n",
      "2020: 1\n",
      "owner: 2\n",
      "april: 1\n",
      "person: 1\n",
      "september: 1\n",
      "1: 1\n",
      "meet: 1\n",
      "team: 1\n",
      "previously: 1\n",
      "artisanal: 1\n",
      "cheesemaker: 1\n",
      "farm: 1\n",
      "vermont: 1\n",
      "reading: 1\n",
      "handpresso: 2\n",
      "nanopresso: 2\n",
      "handheld: 1\n",
      "maker: 2\n",
      "nightmare: 1\n",
      "insist: 1\n",
      "brian: 1\n",
      "lam: 1\n",
      "know: 1\n",
      "aeropress: 3\n",
      "portable: 1\n",
      "frills: 1\n",
      "terrific: 1\n",
      "daniel: 1\n",
      "varghese: 1\n",
      "alex: 1\n",
      "arpaia: 1\n",
      "travel: 1\n",
      "companion: 1\n",
      "laura: 1\n",
      "motley: 1\n",
      "tyler: 1\n",
      "wells: 1\n",
      "lynch: 1\n",
      "dependable: 1\n",
      "dish: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def analyze_frequent_lemmas(html_file):\n",
    "    # Read the HTML file and extract text\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and get the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # Create a list of lemmatized tokens (converted to lowercase) and filter out stop words and punctuation\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "    # Use Counter to count the frequency of each lemma\n",
    "    lemma_frequencies = Counter(lemmas)\n",
    "\n",
    "    # Get the 5 most common lemmas\n",
    "    most_common_lemmas = lemma_frequencies.most_common(5)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Top 5 Most Frequent Lemmas:\")\n",
    "    for lemma, freq in most_common_lemmas:\n",
    "        print(f\"{lemma}: {freq}\")\n",
    "\n",
    "    print(\"\\nLemma Frequencies:\")\n",
    "    for lemma, freq in lemma_frequencies.items():\n",
    "        print(f\"{lemma}: {freq}\")\n",
    "\n",
    "# Example: Analyze frequent lemmas for the saved article HTML\n",
    "html_file = 'cleaned_coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "analyze_frequent_lemmas(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add SpacyTextBlob to the pipeline\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def analyze_sentiment_per_sentence(html_file):\n",
    "    # Read the HTML file and extract text\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and get the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # List to store sentiment scores of sentences\n",
    "    sentence_scores_tokens = []\n",
    "\n",
    "    # Loop over sentences\n",
    "    for sent in doc.sents:\n",
    "        # Get the sentiment polarity score for the sentence\n",
    "        polarity = sent._.blob.sentiment.polarity\n",
    "        \n",
    "        # Add the score to the list of sentence scores\n",
    "        sentence_scores_tokens.append(polarity)\n",
    "\n",
    "    # Plot histogram of sentiment scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sentence_scores_tokens, bins=20, edgecolor='black')\n",
    "    plt.title(\"Histogram of Sentence Sentiment Scores\")\n",
    "    plt.xlabel(\"Sentiment Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Return the sentence scores\n",
    "    return sentence_scores_tokens\n",
    "\n",
    "# Example: Analyze sentiment scores for the saved article HTML\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "sentence_scores_tokens = analyze_sentiment_per_sentence(html_file)\n",
    "\n",
    "# Check the most common range of sentiment scores (based on the histogram)\n",
    "# Most common range of sentiment scores seems to be between -0.2 and 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add SpacyTextBlob to the pipeline\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def analyze_sentiment_using_lemmas(html_file):\n",
    "    # Read the HTML file and extract text\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and get the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # List to store sentiment scores of sentences using lemmas\n",
    "    sentence_scores_lemmas = []\n",
    "\n",
    "    # Loop over sentences\n",
    "    for sent in doc.sents:\n",
    "        # Extract lemmas of the sentence\n",
    "        lemmas = [token.lemma_ for token in sent if not token.is_stop and not token.is_punct]\n",
    "        \n",
    "        # Calculate the sentiment score based on the polarity of each lemma\n",
    "        if lemmas:\n",
    "            # Join lemmas into a sentence string\n",
    "            sentence_text = ' '.join(lemmas)\n",
    "            # Using the sentiment from SpacyTextBlob\n",
    "            sentiment = sent._.blob.sentiment.polarity\n",
    "            sentence_scores_lemmas.append(sentiment)\n",
    "\n",
    "    # Plot histogram of sentiment scores based on lemmas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sentence_scores_lemmas, bins=20, edgecolor='black')\n",
    "    plt.title(\"Histogram of Sentence Sentiment Scores (Using Lemmas)\")\n",
    "    plt.xlabel(\"Sentiment Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Return the sentence scores\n",
    "    return sentence_scores_lemmas\n",
    "\n",
    "# Example: Analyze sentiment scores for the saved article HTML\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "sentence_scores_lemmas = analyze_sentiment_using_lemmas(html_file)\n",
    "\n",
    "# Check the most common range of sentiment scores (based on the histogram)\n",
    "# Most common range of sentiment scores seems to be between -0.2 and 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_cutoff(scores):\n",
    "    # Sort the scores in ascending order\n",
    "    sorted_scores = sorted(scores)\n",
    "    \n",
    "    # Find the index that corresponds to the point where fewer than half the sentences have a higher score\n",
    "    cutoff_index = len(sorted_scores) // 2  # Median index (just below the middle)\n",
    "    \n",
    "    # The cutoff score will be the score at that index\n",
    "    cutoff_score = sorted_scores[cutoff_index]\n",
    "    \n",
    "    return cutoff_score\n",
    "\n",
    "# Example: Calculate cutoff for sentiment scores of tokens and lemmas\n",
    "cutoff_tokens = find_cutoff(sentence_scores_tokens)  # Replace sentence_scores_tokens with the actual scores from tokens\n",
    "cutoff_lemmas = find_cutoff(sentence_scores_lemmas)  # Replace sentence_scores_lemmas with the actual scores from lemmas\n",
    "\n",
    "print(f\"Cutoff Score (tokens): {cutoff_tokens}\")\n",
    "print(f\"Cutoff Score (lemmas): {cutoff_lemmas}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_based_on_tokens(html_file, cutoff_score_tokens):\n",
    "    # Read and parse the HTML as before\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and extract the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # List to store sentences for the summary\n",
    "    sentence_list = []\n",
    "\n",
    "    # Loop over sentences and check their sentiment score (based on tokens)\n",
    "    for sent in doc.sents:\n",
    "        # Get the sentiment score of the sentence (based on tokens)\n",
    "        sentiment = sent._.blob.sentiment.polarity\n",
    "        \n",
    "        # If the sentiment score is greater than the cutoff, add the sentence to the summary\n",
    "        if sentiment > cutoff_score_tokens:\n",
    "            sentence_list.append(sent.text.strip())  # Add the sentence to the list\n",
    "\n",
    "    # Join the sentences together to create the summary\n",
    "    summary = ' '.join(sentence_list)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "# Example: Generate summary for the saved article HTML\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "create_summary_based_on_tokens(html_file, cutoff_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_and_analyze_sentiment(html_file, cutoff_score_tokens):\n",
    "    # Read and parse the HTML as before\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and extract the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # List to store sentences for the summary\n",
    "    sentence_list = []\n",
    "\n",
    "    # Loop over sentences and check their sentiment score (based on tokens)\n",
    "    for sent in doc.sents:\n",
    "        # Get the sentiment score of the sentence (based on tokens)\n",
    "        sentiment = sent._.blob.sentiment.polarity\n",
    "        \n",
    "        # If the sentiment score is greater than the cutoff, add the sentence to the summary\n",
    "        if sentiment > cutoff_score_tokens:\n",
    "            sentence_list.append(sent.text.strip())  # Add the sentence to the list\n",
    "\n",
    "    # Join the sentences together to create the summary\n",
    "    summary = ' '.join(sentence_list)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "    # Calculate the polarity score of the summary\n",
    "    summary_doc = nlp(summary)  # Process the summary with spaCy\n",
    "    summary_polarity = summary_doc._.sentiment.polarity  # Get polarity of the summary\n",
    "\n",
    "    # Count the number of sentences in the summary\n",
    "    num_sentences_in_summary = len(sentence_list)\n",
    "\n",
    "    # Print the polarity and number of sentences\n",
    "    print(f\"\\nPolarity Score of the Summary: {summary_polarity}\")\n",
    "    print(f\"Number of Sentences in the Summary: {num_sentences_in_summary}\")\n",
    "\n",
    "# Example: Generate summary and print its polarity and number of sentences\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "generate_summary_and_analyze_sentiment(html_file, cutoff_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_based_on_lemmas(html_file, cutoff_score_lemmas):\n",
    "    # Read and parse the HTML as before\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and extract the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # List to store sentences for the summary\n",
    "    sentence_list = []\n",
    "\n",
    "    # Loop over sentences and check their sentiment score (based on lemmas)\n",
    "    for sent in doc.sents:\n",
    "        # Get the sentiment score of the sentence (based on lemmas)\n",
    "        sentiment = sent._.blob.sentiment.polarity\n",
    "        \n",
    "        # If the sentiment score is greater than the cutoff, add the sentence to the summary\n",
    "        if sentiment > cutoff_score_lemmas:\n",
    "            sentence_list.append(sent.text.strip())  # Add the sentence to the list\n",
    "\n",
    "    # Join the sentences together to create the summary\n",
    "    summary = ' '.join(sentence_list)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "# Example: Generate summary for the saved article HTML\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "generate_summary_based_on_lemmas(html_file, cutoff_lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_and_analyze_sentiment_lemmas(html_file, cutoff_score_lemmas):\n",
    "    # Read and parse the HTML as before\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse HTML and extract the text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # List to store sentences for the summary\n",
    "    sentence_list = []\n",
    "\n",
    "    # Loop over sentences and check their sentiment score (based on lemmas)\n",
    "    for sent in doc.sents:\n",
    "        # Get the sentiment score of the sentence (based on lemmas)\n",
    "        sentiment = sent._.blob.sentiment.polarity\n",
    "        \n",
    "        # If the sentiment score is greater than the cutoff, add the sentence to the summary\n",
    "        if sentiment > cutoff_score_lemmas:\n",
    "            sentence_list.append(sent.text.strip())  # Add the sentence to the list\n",
    "\n",
    "    # Join the sentences together to create the summary\n",
    "    summary = ' '.join(sentence_list)\n",
    "    \n",
    "    # Print the summary\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "    # Calculate the polarity score of the summary\n",
    "    summary_doc = nlp(summary)  # Process the summary with spaCy\n",
    "    summary_polarity = summary_doc._.blob.sentiment.polarity  # Get polarity of the summary\n",
    "\n",
    "    # Count the number of sentences in the summary\n",
    "    num_sentences_in_summary = len(sentence_list)\n",
    "\n",
    "    # Print the polarity and number of sentences\n",
    "    print(f\"\\nPolarity Score of the Lemma-Based Summary: {summary_polarity}\")\n",
    "    print(f\"Number of Sentences in the Lemma-Based Summary: {num_sentences_in_summary}\")\n",
    "\n",
    "# Example: Generate lemma-based summary and print its polarity and number of sentences\n",
    "html_file = 'coffee_maker_review.html'  # Replace with your saved HTML file path\n",
    "generate_summary_and_analyze_sentiment_lemmas(html_file, cutoff_lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
